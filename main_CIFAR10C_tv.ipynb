{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple, Union, Dict, Callable, List, Iterable\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "#from ColoredMNIST import ColoredMNIST, make_biased_color_samplers\n",
    "from CmnistMLP import CmnistMLP\n",
    "from CmnistMLP0 import CmnistMLP0\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.manifold import TSNE\n",
    "from CMNIST import CMNIST\n",
    "from MLP import MLP\n",
    "from CIFAR10C import CIFAR10C\n",
    "from TV_RESNET18 import TV_RESNET18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_with_tsne(features, labels,path, title = None):\n",
    "\n",
    "    tsne_model = TSNE(n_components=2, random_state=42)\n",
    "    transformed_features = tsne_model.fit_transform(features)\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in unique_labels:\n",
    "        plt.scatter(transformed_features[labels == label, 0], transformed_features[labels == label, 1], label=str(label))\n",
    "\n",
    "    plt.title(\n",
    "        \"t-SNE Visualization of Features\" if title is None \\\n",
    "        else title\n",
    "    )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.savefig(path+f\"{title.replace(' ', '')}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adecs_v2(adecs, tr_labels, tr_features, tr_bias_labels, vl_labels, vl_features, vl_bias_labels, BIAS_AMOUNT, it, STEP, save_results_to):\n",
    "    tr_accuracy = []\n",
    "    vl_accuracy = []\n",
    "    train_TN_rate = []\n",
    "    train_TP_rate = []\n",
    "    val_TN_rate = []\n",
    "    val_TP_rate = []\n",
    "    bias_detected_ratio = []\n",
    "\n",
    "    for i, adec in enumerate(adecs):\n",
    "        tr_idxs = np.where(tr_labels == i)[0]\n",
    "        tr_in_class_samples = tr_features[tr_idxs]\n",
    "        tr_in_class_blabels = tr_bias_labels[tr_idxs]\n",
    "\n",
    "        vl_idxs = np.where(vl_labels == i)[0]\n",
    "        vl_in_class_samples = vl_features[vl_idxs]\n",
    "        vl_in_class_blabels = vl_bias_labels[vl_idxs]\n",
    "\n",
    "        adec.fit(tr_in_class_samples)\n",
    "        tr_predictions = adec.predict(tr_in_class_samples)\n",
    "        vl_predictions = adec.predict(vl_in_class_samples)\n",
    "\n",
    "        # Calcolo delle metriche\n",
    "        tr_accuracy.append(np.count_nonzero(tr_predictions == tr_in_class_blabels) / len(tr_in_class_blabels))\n",
    "        vl_accuracy.append(np.count_nonzero(vl_predictions == vl_in_class_blabels) / len(vl_in_class_blabels))\n",
    "\n",
    "        #y_true=-1\n",
    "        unbiased_samples = tr_in_class_samples[tr_in_class_blabels == -1]\n",
    "        #y_true=1\n",
    "        biased_samples = tr_in_class_samples[tr_in_class_blabels == 1]\n",
    "        #y_pred=-1\n",
    "        predictions_unbiased = tr_predictions[tr_in_class_blabels == -1]\n",
    "        #y_pred=1\n",
    "        predictions_biased = tr_predictions[tr_in_class_blabels == 1]\n",
    "\n",
    "        #y_true=-1\n",
    "        val_unbiased_samples = vl_in_class_samples[vl_in_class_blabels == -1]\n",
    "        #y_true=1\n",
    "        val_biased_samples = vl_in_class_samples[vl_in_class_blabels == 1]\n",
    "        #y_pred=-1\n",
    "        val_predictions_unbiased = vl_predictions[vl_in_class_blabels == -1]\n",
    "        #y_pred=1\n",
    "        val_predictions_biased = vl_predictions[vl_in_class_blabels == 1]\n",
    "\n",
    "        tp_train=np.sum((tr_predictions==1)&(tr_in_class_blabels==1))\n",
    "        fn_train=np.sum((tr_predictions==-1)&(tr_in_class_blabels==1))\n",
    "\n",
    "        tn_train=np.sum((tr_predictions==-1)&(tr_in_class_blabels==-1))\n",
    "        fp_train=np.sum((tr_predictions==1)&(tr_in_class_blabels==-1))\n",
    "\n",
    "        tp_val=np.sum((vl_predictions==1)&(vl_in_class_blabels==1))\n",
    "        fn_val=np.sum((vl_predictions==-1)&(vl_in_class_blabels==1))\n",
    "\n",
    "        tn_val=np.sum((vl_predictions==-1)&(vl_in_class_blabels==-1))\n",
    "        fp_val=np.sum((vl_predictions==1)&(vl_in_class_blabels==-1))\n",
    "\n",
    "\n",
    "\n",
    "        train_TP_rate.append(tp_train/(tp_train+fn_train))\n",
    "        train_TN_rate.append(tn_train/(tn_train+fp_train))\n",
    "\n",
    "        val_TP_rate.append(tp_val/(tp_val+fn_val))\n",
    "        val_TN_rate.append(tn_val/(tn_val+fp_val))\n",
    "\n",
    "        bias_detected_ratio.append( np.count_nonzero(tr_predictions == 1) / len(tr_in_class_blabels))\n",
    "\n",
    "        # Visualizzazione e salvataggio delle immagini (in base a STEP)\n",
    "        print(\"--------------------------------------------------------------------------\")\n",
    "        print(f\"Class {i}: \")\n",
    "        print(f\"\\t -- Train Accuracy (bias_amount={BIAS_AMOUNT}): {tr_accuracy[i]:.3f}\")\n",
    "        print(f\"\\t\\t -- Train true negative rate: {train_TN_rate[i]:.3f}\")\n",
    "        print(f\"\\t\\t -- Train true positive rate: {train_TP_rate[i]:.3f}\")\n",
    "        print(f\"\\t -- Validation Accuracy (bias_amount={BIAS_AMOUNT}): {vl_accuracy[i]:.3f}\")\n",
    "        print(f\"\\t\\t -- Validation true negative rate: {val_TN_rate[i]:.3f}\")\n",
    "        print(f\"\\t\\t -- Validation true positive rate: {val_TP_rate[i]:.3f}\")\n",
    "\n",
    "        visualize_with_tsne(tr_in_class_samples, tr_predictions, save_results_to, title=f\"Class {i}(B={BIAS_AMOUNT}) -- Train Features - Step {it + 1}\")\n",
    "        visualize_with_tsne(vl_in_class_samples, vl_predictions, save_results_to, title=f\"Class {i}(B={BIAS_AMOUNT}) -- Valid Features - Step {it + 1}\")\n",
    "\n",
    "        if STEP == (it + 1):\n",
    "            # Solo nell'ultima iterazione\n",
    "            tr_conf_matrix = confusion_matrix(tr_in_class_blabels, tr_predictions)\n",
    "            disp = ConfusionMatrixDisplay(tr_conf_matrix, display_labels=[\"Unbiased\", \"Biased\"])\n",
    "            disp.plot()\n",
    "            plt.title(f\"Confusion Matrix of Class {i} (B={BIAS_AMOUNT}) -- Train Set\")\n",
    "            plt.savefig(save_results_to + f\"ConfMat_Class_{i}(B={BIAS_AMOUNT})_trainset.png\")\n",
    "            plt.close()\n",
    "\n",
    "            vl_conf_matrix = confusion_matrix(vl_in_class_blabels, vl_predictions)\n",
    "            disp = ConfusionMatrixDisplay(vl_conf_matrix, display_labels=[\"Unbiased\", \"Biased\"])\n",
    "            disp.plot()\n",
    "            plt.title(f\"Confusion Matrix of Class {i} (B={BIAS_AMOUNT}) -- Validation Set\")\n",
    "            plt.savefig(save_results_to + f\"ConfMat_Class_{i}(B={BIAS_AMOUNT})_valset.png\")\n",
    "            plt.close()\n",
    "    return adecs,tr_accuracy, vl_accuracy,train_TN_rate,train_TP_rate,val_TN_rate,val_TP_rate,bias_detected_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
      "--------------------------------------------------------------------------\n",
      "Class 0: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.819\n",
      "\t\t -- Train true negative rate: 0.688\n",
      "\t\t -- Train true positive rate: 0.826\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.813\n",
      "\t\t -- Validation true negative rate: 0.923\n",
      "\t\t -- Validation true positive rate: 0.808\n",
      "--------------------------------------------------------------------------\n",
      "Class 1: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.826\n",
      "\t\t -- Train true negative rate: 0.763\n",
      "\t\t -- Train true positive rate: 0.829\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.828\n",
      "\t\t -- Validation true negative rate: 0.955\n",
      "\t\t -- Validation true positive rate: 0.822\n",
      "--------------------------------------------------------------------------\n",
      "Class 2: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.846\n",
      "\t\t -- Train true negative rate: 0.977\n",
      "\t\t -- Train true positive rate: 0.840\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.863\n",
      "\t\t -- Validation true negative rate: 1.000\n",
      "\t\t -- Validation true positive rate: 0.853\n",
      "--------------------------------------------------------------------------\n",
      "Class 3: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.829\n",
      "\t\t -- Train true negative rate: 0.789\n",
      "\t\t -- Train true positive rate: 0.831\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.831\n",
      "\t\t -- Validation true negative rate: 0.826\n",
      "\t\t -- Validation true positive rate: 0.832\n",
      "--------------------------------------------------------------------------\n",
      "Class 4: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.849\n",
      "\t\t -- Train true negative rate: 1.000\n",
      "\t\t -- Train true positive rate: 0.841\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.827\n",
      "\t\t -- Validation true negative rate: 1.000\n",
      "\t\t -- Validation true positive rate: 0.817\n",
      "--------------------------------------------------------------------------\n",
      "Class 5: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.841\n",
      "\t\t -- Train true negative rate: 0.917\n",
      "\t\t -- Train true positive rate: 0.837\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.834\n",
      "\t\t -- Validation true negative rate: 0.909\n",
      "\t\t -- Validation true positive rate: 0.830\n",
      "--------------------------------------------------------------------------\n",
      "Class 6: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.829\n",
      "\t\t -- Train true negative rate: 0.796\n",
      "\t\t -- Train true positive rate: 0.831\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.804\n",
      "\t\t -- Validation true negative rate: 0.720\n",
      "\t\t -- Validation true positive rate: 0.809\n",
      "--------------------------------------------------------------------------\n",
      "Class 7: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.823\n",
      "\t\t -- Train true negative rate: 0.737\n",
      "\t\t -- Train true positive rate: 0.828\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.814\n",
      "\t\t -- Validation true negative rate: 0.846\n",
      "\t\t -- Validation true positive rate: 0.812\n",
      "--------------------------------------------------------------------------\n",
      "Class 8: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.834\n",
      "\t\t -- Train true negative rate: 0.845\n",
      "\t\t -- Train true positive rate: 0.833\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.818\n",
      "\t\t -- Validation true negative rate: 0.800\n",
      "\t\t -- Validation true positive rate: 0.820\n",
      "--------------------------------------------------------------------------\n",
      "Class 9: \n",
      "\t -- Train Accuracy (bias_amount=0.95): 0.820\n",
      "\t\t -- Train true negative rate: 0.697\n",
      "\t\t -- Train true positive rate: 0.827\n",
      "\t -- Validation Accuracy (bias_amount=0.95): 0.836\n",
      "\t\t -- Validation true negative rate: 0.773\n",
      "\t\t -- Validation true positive rate: 0.839\n",
      "step 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/vito/Marinelli/main_CIFAR10C_tv.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(STEP):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m,it\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m   R18_model_iter\u001b[39m.\u001b[39;49mtrain_model_iter(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     train_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     val_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     test_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     adecs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     ground_truth,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     log_file_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     bias_detected_ratio,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     rand,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mLEARNING_RATE_ITER,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mEPOCHS_ITER,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     bias_amount\u001b[39m=\u001b[39;49mBIAS_AMOUNT\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m   model_save_path_w \u001b[39m=\u001b[39m save_results_to\u001b[39m+\u001b[39mmodel_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B130.251.61.161/home/vito/Marinelli/main_CIFAR10C_tv.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m   \u001b[39m# to save\u001b[39;00m\n",
      "File \u001b[0;32m~/Marinelli/TV_RESNET18.py:241\u001b[0m, in \u001b[0;36mTV_RESNET18.train_model_iter\u001b[0;34m(self, train_loader, val_loader, test_loader, adecs, ground_truth, log_file_path, tr_accuracy_unsup, rand, learning_rate, num_epochs, bias_amount)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m epoch\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m         features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone2(inputs[i])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    242\u001b[0m         p_tensor \u001b[39m=\u001b[39m adecs[l]\u001b[39m.\u001b[39mpredict(features)  \n\u001b[1;32m    243\u001b[0m         p \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(p_tensor\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/Marinelli/TV_RESNET18.py:34\u001b[0m, in \u001b[0;36mTV_RESNET18.backbone2\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet18\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m     33\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet18\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m---> 34\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet18\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Perform average pooling and pass through the final classification layer\u001b[39;00m\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet18\u001b[39m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torchvision/models/resnet.py:93\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m identity \u001b[39m=\u001b[39m x\n\u001b[1;32m     92\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> 93\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m     96\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/Marinelli/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "# 20 iterations (1nn 1ad)\n",
    "BIAS_AMOUNT = 0.95\n",
    "LEARNING_RATE_STEP_0=0.001\n",
    "EPOCHS_STEP_0=30\n",
    "LEARNING_RATE_ITER=0.001\n",
    "STEP=5\n",
    "EPOCHS_ITER=20\n",
    "BATCH_SIZE = 256\n",
    "ground_truth=False\n",
    "rand=False\n",
    "MAX_CONT=0.2\n",
    "\n",
    "model_name=\"test_1\"\n",
    "os.makedirs(f\"./cifar10c_tv/{str(BIAS_AMOUNT).replace('.', '')}_{model_name}\", exist_ok=True)\n",
    "save_results_to = f\"/home/vito/Marinelli/cifar10c_tv/{str(BIAS_AMOUNT).replace('.', '')}_\"+model_name+\"/\"\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  train_set=CIFAR10C(env=\"train\",bias_amount=0.95)\n",
    "  val_set=CIFAR10C(env=\"val\",bias_amount=0.95)\n",
    "  test_set=CIFAR10C(env=\"test\",bias_amount=0.95)\n",
    "\n",
    "  # Objects responsible for loading batches of data during training, validation, and testing\n",
    "  train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=0)\n",
    "  val_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "  test_loader  = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "  R18_model = TV_RESNET18()\n",
    "  model_save_path = save_results_to+model_name+\"_step0.pt\"\n",
    "  \n",
    "  # print(\"step 0\")\n",
    "  # R18_model.train_model_step0(\n",
    "  #   train_loader,\n",
    "  #   val_loader,\n",
    "  #   test_loader,\n",
    "  #   learning_rate=LEARNING_RATE_STEP_0,\n",
    "  #   num_epochs=EPOCHS_STEP_0\n",
    "  #   )\n",
    "  # R18_model.save_model(model_save_path)\n",
    "  \n",
    "  # to upload\n",
    "  R18_model = TV_RESNET18.load_model(model_save_path)\n",
    "  # extraction of the features from the previous trained model\n",
    "  tr_features, tr_labels, tr_bias_labels = R18_model.extract_features(train_loader)\n",
    "  vl_features, vl_labels, vl_bias_labels = R18_model.extract_features(val_loader)\n",
    "  #y_val, y_true, class_contaminations = R18_model.predict_from(val_loader,max_cont=MAX_CONT)\n",
    "  class_contaminations=[0.2 for i in range (10)] ##starting contamination fixed\n",
    "  print(class_contaminations)\n",
    "  # instances of the OC-SVM classifier for unsupervised outlier detection\n",
    "  \n",
    "  adecs = [OneClassSVM(nu=class_contaminations[i]) for i in range(10)]\n",
    "  \n",
    "  adecs,tr_accuracy, vl_accuracy,train_TN_rate,train_TP_rate,val_TN_rate,val_TP_rate,bias_detected_ratio = evaluate_adecs_v2(\n",
    "    adecs,\n",
    "    tr_labels,\n",
    "    tr_features,\n",
    "    tr_bias_labels,\n",
    "    vl_labels,\n",
    "    vl_features,\n",
    "    vl_bias_labels,\n",
    "    BIAS_AMOUNT,\n",
    "    -1,\n",
    "    0,\n",
    "    save_results_to\n",
    "    )\n",
    "\n",
    "  R18_model_iter = TV_RESNET18()\n",
    "  log_file_path=save_results_to+model_name+\".csv\"\n",
    "  learning_rate=LEARNING_RATE_ITER\n",
    "  # trains the model with bias information\n",
    "  # mlp_model_w = CmnistMLP.load_model(model_save_path)\n",
    "  for it in range(STEP):\n",
    "    print(\"step\",it+1)\n",
    "    R18_model_iter.train_model_iter(\n",
    "      train_loader,\n",
    "      val_loader,\n",
    "      test_loader,\n",
    "      adecs,\n",
    "      ground_truth,\n",
    "      log_file_path,\n",
    "      bias_detected_ratio,\n",
    "      rand,\n",
    "      learning_rate=LEARNING_RATE_ITER,\n",
    "      num_epochs=EPOCHS_ITER,\n",
    "      bias_amount=BIAS_AMOUNT\n",
    "      )\n",
    "    \n",
    "    model_save_path_w = save_results_to+model_name+\".pt\"\n",
    "    # to save\n",
    "    R18_model_iter.save_model(model_save_path_w)\n",
    "    # to upload\n",
    "    #mlp_model_w = MnistLP_weighted.load_model(model_save_path_w)\n",
    "    # extraction of the features from the previous trained model\n",
    "    \n",
    "    #y_val, y_true, class_contaminations = R18_model_iter.predict_from(val_loader,max_cont=MAX_CONT)\n",
    "    tr_features, tr_labels, tr_bias_labels = R18_model_iter.extract_features(train_loader)\n",
    "    vl_features, vl_labels, vl_bias_labels = R18_model_iter.extract_features(val_loader)\n",
    "          \n",
    "    class_contaminations=[0.2 for i in range (10)]\n",
    "    print(class_contaminations)\n",
    "\n",
    "    \n",
    "    adecs = [OneClassSVM(nu=class_contaminations[i]) for i in range(10)]\n",
    "\n",
    "    adecs,tr_accuracy, vl_accuracy,train_TN_rate,train_TP_rate,val_TN_rate,val_TP_rate,bias_detected_ratio=evaluate_adecs_v2(\n",
    "      adecs,\n",
    "      tr_labels,\n",
    "      tr_features,\n",
    "      tr_bias_labels,\n",
    "      vl_labels,\n",
    "      vl_features,\n",
    "      vl_bias_labels,\n",
    "      BIAS_AMOUNT,\n",
    "      it,\n",
    "      STEP,\n",
    "      save_results_to\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Marinelli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
